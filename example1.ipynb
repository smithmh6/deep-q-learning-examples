{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning - Example 1\n",
    "This is a tutorial on Deep Q Learning using tensorflow and the OpenAI-gym package taken\n",
    "from:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n",
      "TensorFlow has access to the following devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# See TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# Check for TensorFlow GPU access\n",
    "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Environment\n",
    "The OpenAI-gym package is used to create an environment for the learning agent. The environment contains an action space and an observation space. In the case of 'CartPole-v0', there are two actions that can take place. The cart can go to the left or the right (0, 1). The Discrete action space allows a fixed range of non-negative numbers. The Box space represents an n-dimensional box, so valid observations will be an array of 4 numbers here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 actions in the CartPole-v0 environment.\n",
      "The action space is:\n",
      " Discrete(2)\n",
      "The observation space is:\n",
      " Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Obs. High = [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "Obs. Low = [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "Obs. Shape = (4,)\n"
     ]
    }
   ],
   "source": [
    "# set the environment name\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# make the environment\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "# get the number of actions available\n",
    "# for the agent in the environment\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print(f'There are {num_actions} actions in the {ENV_NAME} environment.')\n",
    "print(f'The action space is:\\n {env.action_space}')\n",
    "print(f'The observation space is:\\n {env.observation_space}')\n",
    "print(f'Obs. High = {env.observation_space.high}')\n",
    "print(f'Obs. Low = {env.observation_space.low}')\n",
    "print(f'Obs. Shape = {env.observation_space.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "The next step is to build the model to be trained. For this example, we will use a 3-layer model with a single input layer and two fully connected layers with 'ReLU' and 'Linear' activation functions, respectively. The output layer shape equates to the number of actions that the agent can take, in this case, 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                80        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 19:46:19.052033: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-16 19:46:19.052148: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# construct the model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(1, ) + env.observation_space.shape),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(num_actions, activation='linear')\n",
    "])\n",
    "\n",
    "# output the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure & Compile the Agent\n",
    "For this example, we will set the policy as Epsilon Greedy and the memory as sequential memory because we want to store the result of actions we performed and the rewards obtained for each action. We then set the agent parameters, the optimizer, and finally compile the model. Some possible metrics for compilation are:\n",
    "1. Mean Squared Error ('mse')\n",
    "2. Mean Absolute Error ('mae')\n",
    "3. Mean Absolute Percentage Error ('mape')\n",
    "4. Cosine Proximity ('cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb#ch0000007?line=4'>5</a>\u001b[0m memory \u001b[39m=\u001b[39m SequentialMemory(limit\u001b[39m=\u001b[39m\u001b[39m50000\u001b[39m, window_length\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb#ch0000007?line=6'>7</a>\u001b[0m \u001b[39m# create the Deep Q agent\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb#ch0000007?line=7'>8</a>\u001b[0m dqn \u001b[39m=\u001b[39m DQNAgent(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb#ch0000007?line=8'>9</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb#ch0000007?line=9'>10</a>\u001b[0m     nb_actions\u001b[39m=\u001b[39;49mnum_actions,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb#ch0000007?line=10'>11</a>\u001b[0m     memory\u001b[39m=\u001b[39;49mmemory,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb#ch0000007?line=11'>12</a>\u001b[0m     nb_steps_warmup\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb#ch0000007?line=12'>13</a>\u001b[0m     target_model_update\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb#ch0000007?line=13'>14</a>\u001b[0m     policy\u001b[39m=\u001b[39;49mpolicy\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb#ch0000007?line=14'>15</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb#ch0000007?line=16'>17</a>\u001b[0m \u001b[39m# set the optimizer with the learning rate\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/heathsmith/repos/devops/Python_Development/deep_q_learning/example1.ipynb#ch0000007?line=17'>18</a>\u001b[0m opt \u001b[39m=\u001b[39m Adam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n",
      "File \u001b[0;32m~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/rl/agents/dqn.py:108\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[0;34m(self, model, policy, test_policy, enable_double_dqn, enable_dueling_network, dueling_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/rl/agents/dqn.py?line=104'>105</a>\u001b[0m \u001b[39msuper\u001b[39m(DQNAgent, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/rl/agents/dqn.py?line=106'>107</a>\u001b[0m \u001b[39m# Validate (important) input.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/rl/agents/dqn.py?line=107'>108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model\u001b[39m.\u001b[39moutput, \u001b[39m'\u001b[39m\u001b[39m__len__\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39;49m(model\u001b[39m.\u001b[39;49moutput) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/rl/agents/dqn.py?line=108'>109</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mModel \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m has more than one output. DQN expects a model that has a single output.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(model))\n\u001b[1;32m    <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/rl/agents/dqn.py?line=109'>110</a>\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39m_keras_shape \u001b[39m!=\u001b[39m (\u001b[39mNone\u001b[39;00m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions):\n",
      "File \u001b[0;32m~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/keras/engine/keras_tensor.py:221\u001b[0m, in \u001b[0;36mKerasTensor.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/keras/engine/keras_tensor.py?line=219'>220</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/keras/engine/keras_tensor.py?line=220'>221</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mKeras symbolic inputs/outputs do not \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/keras/engine/keras_tensor.py?line=221'>222</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mimplement `__len__`. You may be \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/keras/engine/keras_tensor.py?line=222'>223</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mtrying to pass Keras symbolic inputs/outputs \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/keras/engine/keras_tensor.py?line=223'>224</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mto a TF API that does not register dispatching, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/keras/engine/keras_tensor.py?line=224'>225</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mpreventing Keras from automatically \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/keras/engine/keras_tensor.py?line=225'>226</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mconverting the API call to a lambda layer \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/keras/engine/keras_tensor.py?line=226'>227</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39min the Functional Model. This error will also get raised \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///~/repos/devops/Python_Development/deep_q_learning/env/lib/python3.9/site-packages/keras/engine/keras_tensor.py?line=227'>228</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mif you try asserting a symbolic input/output directly.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly."
     ]
    }
   ],
   "source": [
    "# set the policy\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "# set the memory\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# create the Deep Q agent\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=num_actions,\n",
    "    memory=memory,\n",
    "    nb_steps_warmup=250,\n",
    "    target_model_update=1e-2,\n",
    "    policy=policy\n",
    ")\n",
    "\n",
    "# set the optimizer with the learning rate\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "# compile the agent with mean absolute error (mae) \n",
    "dqn.compile(opt, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Now the agent can be trained using the .fit() method. Here we will visualize the training, but this can slow down performance for larger training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: 1.0000\n",
      "done, took 87.786 seconds\n"
     ]
    }
   ],
   "source": [
    "# set the number of training steps\n",
    "n_steps = 10000\n",
    "\n",
    "# train the model\n",
    "history = dqn.fit(\n",
    "    env,\n",
    "    nb_steps=n_steps,\n",
    "    visualize=False,\n",
    "    verbose=1,\n",
    "    log_interval=n_steps\n",
    ")\n",
    "\n",
    "# close the environment after training\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Lastly, we can test the model to see how well it was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 99.000, steps: 99\n",
      "Episode 2: reward: 96.000, steps: 96\n",
      "Episode 3: reward: 103.000, steps: 103\n",
      "Episode 4: reward: 102.000, steps: 102\n",
      "Episode 5: reward: 107.000, steps: 107\n",
      "Episode 6: reward: 115.000, steps: 115\n",
      "Episode 7: reward: 164.000, steps: 164\n",
      "Episode 8: reward: 111.000, steps: 111\n",
      "Episode 9: reward: 103.000, steps: 103\n",
      "Episode 10: reward: 93.000, steps: 93\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "dqn.test(env, nb_episodes=10, visualize=True)\n",
    "\n",
    "# close the environment after testing\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "896258f142e21b19c601acee14f8672a5a898de0f5771e2b10ac8a3a1de1dc72"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
